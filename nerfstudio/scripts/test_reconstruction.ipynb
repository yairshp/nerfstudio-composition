{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "from nerfstudio.cameras.cameras import Cameras\n",
    "from nerfstudio.field_components.field_heads import FieldHeadNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"/root/yairshp/output_models/transformations/outputs/table/mipnerf/2023-09-27_212748/config.yml\"\n",
    "config_path = Path(\"/\", *config_path.split(\"/\"))\n",
    "checkpoint_path = \"/root/yairshp/output_models/transformations/outputs/table/mipnerf/2023-09-27_212748/nerfstudio_models\"\n",
    "\n",
    "fg_config_path = \"/root/yairshp/output_models/transformations/outputs/ficus/mipnerf/2023-09-23_212151/config.yml\"\n",
    "fg_config_path = Path(\"/\", *fg_config_path.split(\"/\"))\n",
    "fg_checkpoint_path = \"/root/yairshp/output_models/transformations/outputs/ficus/mipnerf/2023-09-23_212151/nerfstudio_models\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pipeline, _, _ = eval_setup(config_path, eval_num_rays_per_chunk=None, test_mode=\"inference\", checkpoint_path=checkpoint_path)\n",
    "_, fg_pipeline, _, _ = eval_setup(fg_config_path, eval_num_rays_per_chunk=None, test_mode=\"inference\", checkpoint_path=fg_checkpoint_path)\n",
    "pipeline.datamanager.setup_train()\n",
    "fg_pipeline.datamanager.setup_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_index = torch.tensor(119)\n",
    "camera_transforms = pipeline.datamanager.train_camera_optimizer(current_index.unsqueeze(dim=0))\n",
    "current_camera = pipeline.datamanager.train_dataparser_outputs.cameras[current_index].to('cuda')\n",
    "modified_camera = Cameras(\n",
    "    fx=torch.tensor([[548.9938],[548.9938]]).to('cuda'),\n",
    "    fy=torch.tensor([[548.9938],[548.9938]]).to('cuda'),\n",
    "    cx=torch.tensor(256.).to('cuda'),\n",
    "    cy=torch.tensor(256.).to('cuda'),\n",
    "    camera_to_worlds=current_camera.camera_to_worlds,\n",
    "    camera_type=current_camera.camera_type,\n",
    "    times=current_camera.times\n",
    ")\n",
    "current_ray_bundle = modified_camera.generate_rays(torch.tensor(list(range(1))).unsqueeze(-1), camera_opt_to_camera=camera_transforms, aabb_box=None)\n",
    "fg_ray_bundle = modified_camera.generate_rays(torch.tensor(list(range(1))).unsqueeze(-1), camera_opt_to_camera=camera_transforms, aabb_box=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_camera_transformations = camera_transforms.clone()\n",
    "\n",
    "fg_translation = fg_camera_transformations.clone()\n",
    "fg_translation[0][3] = 0.\n",
    "fg_translation[1][3] = -1.33\n",
    "fg_translation[2][3] = 0.\n",
    "\n",
    "fg_scale = fg_camera_transformations.clone()\n",
    "fg_scale[0][0] = 1.1\n",
    "fg_scale[1][1] = 1.1\n",
    "fg_scale[2][2] = 1.1\n",
    "\n",
    "fg_rotation_z = fg_camera_transformations.clone()\n",
    "fg_rotation_z[0][0] = math.cos(math.pi / 4)\n",
    "fg_rotation_z[1][1] = math.cos(math.pi / 4)\n",
    "fg_rotation_z[0][1] = math.sin(math.pi / 4)\n",
    "fg_rotation_z[1][0] = -math.sin(math.pi / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = pipeline.model.get_outputs_for_camera_ray_bundle(current_ray_bundle, fg_pipeline=fg_pipeline, fg_camera_ray_bundle=fg_ray_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (outputs['rgb_coarse'].cpu().numpy() * 255).astype(np.uint8)\n",
    "image = Image.fromarray(image)\n",
    "image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_rays_per_chunk = pipeline.model.config.eval_num_rays_per_chunk\n",
    "# image_height, image_width = current_ray_bundle.origins.shape[:2]\n",
    "# num_rays = len(current_ray_bundle)\n",
    "# # outputs_lists = defaultdict(list)\n",
    "# rgb_fine_outputs = []\n",
    "# with torch.no_grad():\n",
    "#     for i in range(0, num_rays, num_rays_per_chunk):\n",
    "#         start_idx = i\n",
    "#         end_idx = i + num_rays_per_chunk\n",
    "#         ray_bundle = current_ray_bundle.get_row_major_sliced_ray_bundle(start_idx, end_idx)\n",
    "#         if pipeline.model.collider is not None:\n",
    "#             ray_bundle = pipeline.model.collider(ray_bundle)\n",
    "#         if pipeline.model.field is None:\n",
    "#                 raise ValueError(\"populate_fields() must be called before get_outputs\")\n",
    "#         ray_samples_uniform = pipeline.model.sampler_uniform(ray_bundle)\n",
    "#         field_outputs_coarse = pipeline.model.field.forward(ray_samples_uniform)\n",
    "#         weights_coarse = ray_samples_uniform.get_weights(field_outputs_coarse[FieldHeadNames.DENSITY])\n",
    "#         rgb_coarse = pipeline.model.renderer_rgb(rgb=field_outputs_coarse[FieldHeadNames.RGB], weights=weights_coarse)\n",
    "#         ray_samples_pdf = pipeline.model.sampler_pdf(ray_bundle, ray_samples_uniform, weights_coarse)\n",
    "#         field_outputs_fine = pipeline.model.field.forward(ray_samples_pdf)\n",
    "#         weights_fine = ray_samples_pdf.get_weights(field_outputs_fine[FieldHeadNames.DENSITY])\n",
    "#         rgb_fine = pipeline.model.renderer_rgb(\n",
    "#             rgb=field_outputs_fine[FieldHeadNames.RGB],\n",
    "#             weights=weights_fine,\n",
    "#         )\n",
    "#         rgb_fine_outputs.append(rgb_fine)\n",
    "#     rgb_fine = torch.cat(rgb_fine_outputs).view(image_height, image_width, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = (rgb_fine.cpu().numpy() * 255).astype(np.uint8)\n",
    "# image = Image.fromarray(image)\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
